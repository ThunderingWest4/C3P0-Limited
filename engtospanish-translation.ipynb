{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport unicodedata\nimport re\nimport io\n\ndef getData(path):\n    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n    # n_total = 118964\n    pairs = [[preprocess(x) for x in l.split('\\t')] for l in lines]\n    en, sp = zip(*pairs) #make tuples from pairs\n\n    en_tensor, en_map = tokenize(en)\n    sp_tensor, sp_map = tokenize(sp)\n\n    return en_tensor, sp_tensor, en_map, sp_map\n\ndef uni_to_ascii(s) -> str:\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn') # ensuring it's not an accent\n\ndef preprocess(w) -> str:\n    w = uni_to_ascii(w.lower().strip())\n\n    # make space between word and punct\n    w = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", w) # substitutes captured string thing (referenced w \\1) with that thing + space\n    # get rid of multiple space seq things\n    w = re.sub(r'[\" \"]+', \" \", w)\n\n    # keep only letters and punct\n    w = re.sub(r\"[^a-zA-Z?.!,¿¡]+\", \" \", w).strip()\n\n    # add start + end token for model\n    w = '<s> ' + w + ' <e>'\n    \n    return w\n\ndef tokenize(lang) -> (tf.Tensor, tf.keras.preprocessing.text.Tokenizer):\n    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n    lang_tokenizer.fit_on_texts(lang)\n\n    tensor = lang_tokenizer.texts_to_sequences(lang)\n    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n\n    return tensor, lang_tokenizer\n\ndef detokenize(sequence, lang_map) -> str:\n    toRet = \"\"\n    for x in sequence:\n        if x != 0:\n            toRet += lang_map.index_word[x] + \" \"\n    return toRet","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef sep_data(enc_og, dec_og, n_eng_words, n_es_words):\n    \n    \n    n_pairs = len(enc_og)\n    max_eng = len(enc_og[0])\n    max_es = len(dec_og[0])\n    \n    enc_inp = np.zeros((n_pairs, max_eng, n_eng_words), dtype=int)\n    dec_inp = np.zeros((n_pairs, max_es, n_es_words), dtype=int)\n    dec_out = np.zeros((n_pairs, max_es, n_es_words), dtype=int) #going to be same dims as dec_inp but off by a timestep\n    # so a specific index in dec_out would correspond to index-1 in dec_inp\n    \n    # actually putting data into the arrays no\n    \n    for i, (inp, targ) in enumerate(zip(enc_og, dec_og)):\n        \n        for t, c in enumerate(inp): # english data\n            enc_inp[i, t, c] = 1 # c represents the index/token/code thing for a word so we're basically going to that index\n            # and saying \"hey, there's x word present at this timestamp in this data pair\"\n            # no need to add spacces bc it's doing words so by default, after predictions and joined, spaces will be added in\n        \n        for t, c in enumerate(targ): # spanish data\n            dec_inp[i, t, int(c)] = 1\n            if t>0:\n                # dec out will be ahead by 1 timestep and not include first word\n                # so second word of input will be first of output\n                dec_out[i, t-1, int(c)] = 1\n                         \n    \n    return (enc_inp, dec_inp, dec_out)","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras as keras\nimport numpy as np\n\n\"\"\"\nBorrowed code from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n(modified some of it but used that site for base)\n\"\"\"\n\n\nclass DataGenerator(keras.utils.Sequence):\n# class DataGenerator():\n\n    def __init__(self, list_IDs, n_en, n_es, batch_size=32, shuffle=True):\n        # Initializing the generator\n        \n#         self.dim=dim # This is actually necessary for the keras.utils.Sequence, need to figure out the dims of the batch/data returned\n        self.batch_size=2 # files are already batched, we need to load one of them\n        self.dim = (32, 51, 53)\n#         self.labels=labels\n        self.list_id=list_IDs\n#         self.n_chan=n_channels\n#         self.n_class=n_classes\n        self.shuffle=shuffle\n        \n        self.n_eng = n_en\n        self.n_esp = n_es\n        \n        self.on_epoch_end()\n        \n    def on_epoch_end(self):\n        self.idx = np.arange(len(self.list_id))\n        \n        # Makes sure that batches change between epochs\n        if self.shuffle:\n            np.random.shuffle(self.idx)\n            \n    def __len__(self):\n        return int(np.floor(len(self.list_id)) // self.batch_size) # n_epochs per batch | the // operator divides but turns it into an int as well w/o the explicit int(x) call\n    \n    def __getitem__(self, idx):\n        # generate indices of bat\n        idxs = self.idx[(idx*self.batch_size):((idx+1)*self.batch_size)]\n#         print(\"idxs \" + str(idxs))\n    \n        temp_idx = [self.list_id[k] for k in idxs]\n        \n        enc, d_in, d_out = self.__data_generation(temp_idx)\n        \n#         for i in range(len(enc)):\n#             yield ([enc[i], d_in[i]], d_out[i])\n        \n        return ([enc, d_in], d_out)\n    \n    \n    def __data_generation(self, temp_ids):\n            \n        \"\"\"\n        The way this will work is basically by loading in the data at the id in a saved np file\n        Then we'll read it in as the 2d matrix (a list of tensors where each tensor is a list of indices/values) and transform it into a 3d matrix in the style of the sep_data method\n        \"\"\"\n        \n#         en = np.empty((self.bat, *self.dim, self.n_chan))\n#         es = np.empty((self.bat, *self.dim, self.n_chan))\n        en = []\n        es = []\n        \n        # data gen\n        for i, ID in enumerate(temp_ids):\n            # store sample\n            en.append(np.load('../input/spa-eng-separated/archive/' + str(ID) + '_en.npy'))\n            es.append(np.load('../input/spa-eng-separated/archive/' + str(ID) + '_es.npy'))\n        \n#         print(f\"en {str(en)} es {str(es)}\")\n        # data loading and en/es declaration is working correctly\n        \n#         enc_in = np.ndarray([])\n#         dec_in = np.ndarray([])\n#         dec_out = np.ndarray([])\n#         count = 0\n    \n#         for i, x in enumerate(en):\n#             e_temp, d_temp, o_temp = sep_data(en[i], es[i], self.n_eng, self.n_esp)\n#             # returned shapes: \n            \n#             # e_temp: (32, 51, 12933) | np.ndarray | encoder input\n#             # d_temp: (32, 53, 24794) | np.ndarray | decoder input\n#             # o_temp: (32, 53, 24794) | np.ndarray | decoder output\n\n# #             print(e_temp.shape, d_temp.shape, o_temp.shape)\n# #             print(f\"temp enc {str(e_temp)} temp dec in {str(d_temp)} temp dec out {str(o_temp)}\")\n#             # this is working as well, has them big [0, 1] arrays with the time stuff. So where's the error?\n#             # seems like the only place is in the np.append. But idk what's wrong\n#             # oh my god i'm stupid, i wasn't actually appending anything. np.append returns a copy so you have to do new_arr = np.append(old_arr, val)\n#             # for some reason this strips the dims and just makes a long 1dim arr\n#             enc_in = np.append(enc_in, e_temp)\n#             dec_in = np.append(dec_in, d_temp)\n#             dec_out = np.append(dec_out, o_temp)\n# #             print(enc_in)\n#             count += 1\n#             print(count)\n            \n#         enc_in.astype('int')\n#         dec_in.astype('int')\n#         dec_out.astype('int')\n#         print(\"enc in \" + str(enc_in) + \" dec in \" + str(dec_in) + \" dec out \"+ str(dec_out))\n        e_temp, d_temp, o_temp = sep_data(en[i], es[i], self.n_eng, self.n_esp)\n\n        return e_temp, d_temp, o_temp\n    \n#         return enc_in, dec_in, dec_out\n        \n        \n  ","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras as keras\nimport keras.layers as layers\nimport numpy as np\n\nclass EnDe2():\n    def __init__(self, inp_size, targ_size, embedding_dim, units, bat, input_vocab, target_vocab):\n        \n        self.bat_size = bat\n        \n        # input -> encoder embedding -> encoder GRU -> \n        # save states -> decoder input -> decoder GRU w encoder states -> \n        # decoder LSTM -> dense w softmax activ\n        \n#        # e_temp: (32, 51, 12933) | np.ndarray | encoder input\n#        # d_temp: (32, 53, 24794) | np.ndarray | decoder input\n#        # o_temp: (32, 53, 24794) | np.ndarray | decoder output\n        \n        enc_in = layers.Input(shape=(inp_size, input_vocab), batch_size=bat)\n#         print(input_vocab, embedding_dim)\n#         enc_out = layers.Embedding(input_dim=input_vocab+1, output_dim=embedding_dim//2)(enc_in)\n        enc = keras.layers.LSTM(embedding_dim, return_state=True)\n        enc_out, state_h, state_c = enc(enc_in)\n#         enc_out, state = layers.GRU(units//2, \n#                                     return_state=True,\n#                                     recurrent_initializer='glorot_uniform')(enc_out)\n\n        enc_states = [state_h, state_c]\n\n        dec_in = layers.Input(shape=(targ_size, target_vocab), batch_size=bat)\n#         print(target_vocab, embedding_dim\n#         dec_out = layers.Embedding(input_dim=target_vocab+1, output_dim=embedding_dim//2)(dec_in)\n#         dec_out = layers.GRU(units//2)(dec_out, initial_state=state)\n#         dec_out = layers.Dense(targ_size, activation='softmax')(dec_out)\n        dec_lstm = keras.layers.LSTM(embedding_dim, \n                                     return_sequences=True, \n                                     return_state=True)\n        dec_out, _, _ = dec_lstm(dec_in, initial_state=enc_states)\n        dec_dense = keras.layers.Dense(target_vocab, activation='softmax')\n        dec_out = dec_dense(dec_out)\n        \n        self.model = keras.models.Model([enc_in, dec_in], dec_out)\n        \n        self.model.compile(optimizer='adam', loss='categorical_crossentropy')\n        self.model.summary()\n        \n    def train(self, train_gen, epochs):\n        self.model.compile(\n            optimizer=\"rmsprop\", \n            loss=\"categorical_crossentropy\", \n            metrics=[\"accuracy\"]\n        )\n#         self.model.fit(\n#             [e, d_in],\n#             d_out,\n#             batch_size=self.bat_size,\n#             epochs=epochs,\n#             validation_split=0.2, \n#         )\n        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n            filepath=\"s2s\",\n            save_weights_only=True,\n            monitor='val_accuracy',\n            mode='max',\n            save_best_only=True\n        )\n\n        for i in range(epochs):\n            \"\"\"\n            APPARENTLY callbacks doesn't work so the data doesn't save\n            and considering kaggle sessions last 9 hours max and this takes 2 hours per epoch, we *need* checkpoints very badly\n            so yeah\n            crappy but hopefully workable solution\n            \"\"\"\n            self.model.fit(\n                x=train_gen, # train_gen will return tuple ([encoder_in, decoder_in], decoder_out)\n                epochs=1, \n                workers = 5,\n                use_multiprocessing = True, \n                callbacks = [model_checkpoint_callback]\n\n            )\n            \n            self.model.save(\"s2s\")\n        ","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n#import dataset\n#import EncoderDecoder\nfrom termcolor import colored\n#import network\nimport os.path\nimport os\nimport json\nimport random\n\n\nprint(colored(\"Successfully imported packages\", \"green\"))\n\nCITATION=\"\"\"\n@inproceedings{\nTiedemann2012ParallelData,\nauthor = {Tiedemann, J},\ntitle = {Parallel Data, Tools and Interfaces in OPUS},\nbooktitle = {LREC}\nyear = {2012}\n}\n\"\"\"\n\n# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# # instantiating the model in the strategy scope creates the model on the TPU\n# with tpu_strategy.scope():\n\n# Download the file\n#path_to_zip = tf.keras.utils.get_file(\n#    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n#    extract=True)\n\nen, es, en_map, es_map = getData(\"../input/spaeng/spa.txt\")\n# en_f = []\n# es_f = []\n\n# # espanol dict is larger than the eng dict so we pad for even\n# target_length = len(en[0]) if len(en[0]) > len(es[0]) else len(es[0])\n# for i in range(len(en)): # they have the same amount of samples so we can use one index\n#     en_f.append(np.append(en[i], [0]*(target_length - len(en[i]))))\n#     es_f.append(np.append(es[i], [0]*(target_length - len(es[i]))))\n\n#     en_train = en_f[0:100000]\n#     es_train = es_f[0:100000]\n#     en_eval = en_f[100000:]\n#     es_eval = es_f[100000:]\n\nBATCH_SIZE=32\n#     BUFFER_SIZE = len(en_train)\nEMBEDDING_DIM = 512\n#     STEP_EPOCH = len(en_train)//BATCH_SIZE\nEPOCHS = 30\nUNITS = 1024\nVOCAB_INP_SIZE = len(en[0])\nVOCAB_OUT_SIZE = len(es[0])\n\n# VOCAB_INP_SIZE = 51\n# VOCAB_OUT_SIZE = 53\n\n# train_ds = tf.data.Dataset.from_tensor_slices((en_train, es_train)).shuffle(BUFFER_SIZE)\n# train_ds = train_ds.batch(BATCH_SIZE, drop_remainder=True)\n\n# eval_ds = tf.data.Dataset.from_tensor_slices((en_eval, es_eval)).shuffle(BUFFER_SIZE)\n# eval_ds = eval_ds.batch(BATCH_SIZE, drop_remainder=True)\n\n# full_ds = tf.data.Dataset.from_tensor_slices((en_f, es_f)).shuffle(BUFFER_SIZE)#.batch(BATCH_SIZE, drop_remainder=True)\n\n\n# print(colored(\"Train and Eval datasets created\", \"green\"))\n\n# print(detokenize(en_train[0], en_map) + \" | \" + detokenize(es_train[0], es_map))\n\n# for i in range(0, 52):\n#     print(detokenize([i], en_map))\n\n# for i in range(0, 54):\n#     print(detokenize([i], es_map))\n\n\"\"\"\nEncDecModel = EncDec(VOCAB_INP_SIZE, VOCAB_OUT_SIZE, EMBEDDING_DIM, UNITS, BATCH_SIZE, en_map, es_map)\nEncDecModel.train(train_ds, EPOCHS, STEP_EPOCH)\ntest = preprocess(\"hi how are you\")\nprint(f\"Preprocessed: {test}\")\ntestseq = en_map.text_to_sequences(test)\nencout, weights = EncDecModel.encoder(testseq, hidden_init(BATCH_SIZE, UNITS))\nend = EncDecModel.decoder(encout)\nprint(f\"Output: {es_map.sequences_to_texts(end)}\")\n\"\"\"\n# print(full_ds)\nen_config = en_map.get_config()\nes_config = es_map.get_config()\nn_eng = len(json.loads(en_config['word_counts']).keys()) # going into the config dict, taking the dict with the word counts, and taking the n of keys to get the overall number of words because apparently my other method is broken :/\nn_es = len(json.loads(es_config['word_counts']).keys())\n\n# dec_out = [np.append(x[1:], [0]) for x in es_f]\n\n# method header\n# sep_data(enc_og, dec_og, n_eng_words, n_es_words)\n\n# en_f, es_f, dec_out = sep_data(en, es, n_eng, n_es)\n# print(dec_out[0])\n\nED2 = EnDe2(VOCAB_INP_SIZE, VOCAB_OUT_SIZE, EMBEDDING_DIM, UNITS, BATCH_SIZE, n_eng, n_es)\nprint(colored(\"About to start training\", \"green\"))\n# print(len(en_f), len(es_f), len(dec_out))\n# print(en_f[0:5], es_f[0], dec_out[0])\n# ED2.train(en_f[0:200], es_f[0:200], dec_out[0:200], EPOCHS)\n\n\"\"\"\nTHINGS TO ADD: \n\nPARAMS W/ DIM, BATCH SIZE, N_CLASSES, N_CHANNELS, SHUFFLE\nPARTITION W N_IDS\nLABELS W N_LABELS\ntrain_gen = DataGenerator(partition['train'], labels, **params)\neval_gen = DataGenerator(partition['train'], labels, **params)\n\nED2.train(train_gen, eval_gen)\n\nMAKE SURE THAT THE DATA GENERATOR TECHNIQUE WILL WORK WITH THE KERAS FUNCTIONAL API THING\nIF NOT, SCREW AROUND WITH THE FUNCTIONAL API AND INPUTS UNTIL IT DOES\n\n\"\"\"\nlist_ids = sorted(np.load('../input/spa-eng-separated/archive/list_ids.npy'))\ntrain_gen = DataGenerator(list_ids, n_eng, n_es)\n# print(train_gen.__getitem__(0))\nED2.train(train_gen, EPOCHS)\n\ntest_string = \"hi how are you doing\"\ntokenized_string = en_map.texts_to_sequences(test_string)\nresult = ED2.model.predict(tokenized_string)\ntranslated = es_map.sequences_to_texts(result)\nprint(f\"Model translated {test_string} (eng) to {translated} (esp)\")\n\n# NMTAttn = network.NMTAttn(VOCAB_INP_SIZE, VOCAB_OUT_SIZE, UNITS, n_encoder=3, n_decoder=3, n_attn_heads=1, dropout=0.03, mode='test')\n# NMTAttn.model(np.array([1, 2, 3]))","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[32mSuccessfully imported packages\u001b[0m\nModel: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(32, 51, 12933)]    0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(32, 53, 24794)]    0                                            \n__________________________________________________________________________________________________\nlstm (LSTM)                     [(32, 512), (32, 512 27537408    input_1[0][0]                    \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(32, 53, 512), (32, 51828736    input_2[0][0]                    \n                                                                 lstm[0][1]                       \n                                                                 lstm[0][2]                       \n__________________________________________________________________________________________________\ndense (Dense)                   (32, 53, 24794)      12719322    lstm_1[0][0]                     \n==================================================================================================\nTotal params: 92,085,466\nTrainable params: 92,085,466\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\u001b[32mAbout to start training\u001b[0m\n1858/1858 [==============================] - 6037s 3s/step - loss: 0.8639 - accuracy: 0.8769\n 220/1858 [==>...........................] - ETA: 1:28:31 - loss: 0.7336 - accuracy: 0.8915","output_type":"stream"}]},{"cell_type":"code","source":"# # Saving Data\n# import numpy as np\n# import os\n\n# # Clearing output to make sure we don't crash the notebook by writing too much to disc lol\n# # for file in os.walk(\"./\"):\n# #     print(file)\n# #     os.remove(file)\n\n# DataGenNeeded = False\n# batch = 32\n# # leaving code so that i can generate new dataset thing whenever i need\n\n# if DataGenNeeded:\n\n#     en, es, en_map, es_map = getData(\"../input/spaeng/spa.txt\")\n\n#     list_ids = np.array([])\n#     counter = 0\n\n#     for i in range(0, len(en), batch):\n#         if i+batch < len(en):\n#             np.save(f\"{counter}_en.npy\", en[i:i+batch])\n#             np.save(f\"{counter}_es.npy\", es[i:i+batch])\n#             np.append(list_ids, counter)\n#             print(f\"Saved values {i} to {i+batch} in file with id {counter}\")\n\n#         else: \n#             continue\n#             # ignore the files with batch size <32\n\n#         counter += 1\n\n#     np.save(\"list_ids.npy\", list_ids)\n#     print(\"Data Save complete\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# ID=\"0\"\n# a = np.load('../input/spa-eng-separated/' + ID + '_en.npy')\n# b = np.load('../input/spa-eng-separated/' + ID + '_es.npy')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}