{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install numpy tensorflow --use-feature=2020-resolver termcolor ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport unicodedata\nimport re\nimport io\n\ndef getData(path):\n    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n    # n_total = 118964\n    pairs = [[preprocess(x) for x in l.split('\\t')] for l in lines]\n    en, sp = zip(*pairs) #make tuples from pairs\n\n    en_tensor, en_map = tokenize(en)\n    sp_tensor, sp_map = tokenize(sp)\n\n    return en_tensor, sp_tensor, en_map, sp_map\n\ndef uni_to_ascii(s) -> str:\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn') # ensuring it's not an accent\n\ndef preprocess(w) -> str:\n    w = uni_to_ascii(w.lower().strip())\n\n    # make space between word and punct\n    w = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", w) # substitutes captured string thing (referenced w \\1) with that thing + space\n    # get rid of multiple space seq things\n    w = re.sub(r'[\" \"]+', \" \", w)\n\n    # keep only letters and punct\n    w = re.sub(r\"[^a-zA-Z?.!,¿¡]+\", \" \", w).strip()\n\n    # add start + end token for model\n    w = '<s> ' + w + ' <e>'\n    \n    return w\n\ndef tokenize(lang) -> (tf.Tensor, tf.keras.preprocessing.text.Tokenizer):\n    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n    lang_tokenizer.fit_on_texts(lang)\n\n    tensor = lang_tokenizer.texts_to_sequences(lang)\n    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n\n    return tensor, lang_tokenizer\n\ndef detokenize(sequence, lang_map) -> str:\n    toRet = \"\"\n    for x in sequence:\n        if x != 0:\n            toRet += lang_map.index_word[x] + \" \"\n    return toRet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers as layers\nimport tensorflow.keras as keras\nimport tensorflow as tf\nimport numpy as np\nimport time\nfrom termcolor import colored\nimport sys\n\nclass EncDec():\n\n    def __init__(self, input_vocab, target_vocab, embedding_dim, units, batch_size, inpmap, targmap):\n\n        # constructs network\n        \n        hidden = hidden_init(batch_size, units)\n        self.input_dim = input_vocab\n        self.out_dim = target_vocab\n        self.batch_size = batch_size\n        self.embed_dim = embedding_dim\n        self.units = units\n        self.inmap = inpmap\n        self.outmap = targmap\n\n        self.encoder = EncDec.Encoder(input_vocab, embedding_dim, units, batch_size)\n        self.decoder = EncDec.Decoder(target_vocab, embedding_dim, units, batch_size)\n\n        print(colored(\"Encoder and Decoder models created! Ready for training\", \"green\"))\n\n    @tf.function\n    def train(self, data, epochs, steps_per_epoch):\n        self.optimizer = keras.optimizers.Adam(0.01)\n        self.loss_obj = keras.losses.SparseCategoricalCrossentropy(\n            from_logits=True,\n            reduction='none')\n        \n        print(colored(f\"Beginning training for {epochs} epochs\", \"green\"))\n            \n        for ep in range(epochs):\n            start = time.time()\n\n            enchid = hidden_init(self.batch_size, self.units)\n            total_loss = 0\n\n            for (batch, (inp, targ)) in enumerate(data.take(steps_per_epoch)):\n\n                # print(inp.shape, targ.shape)\n                batch_loss = 0\n                for i in range(batch):\n                    batch_loss += self.train_step(inp[i], targ[i], enchid)\n                total_loss += batch_loss\n\n                if(batch%100 == 0): \n                    print(f\"Epoch {ep+1} | Batch {batch} | Loss {batch_loss}\")\n\n            print(colored(f\"Epoch {ep+1} completed | Loss {total_loss/steps_per_epoch}\", \"green\"))\n            print(f\"Time for epoch {ep+1}: {time.time()-start} seconds\")\n                \n        print(colored(\"Training completed!\", \"green\"))\n\n\n            \n    def loss(self, ans, pred):\n        mask = tf.math.logical_not(tf.math.equal(ans, 0))\n        #print(str(ans.shape) + \" \" + str(pred.shape))\n        \n        loss = self.loss_obj(ans, pred)\n        mask = tf.cast(mask, dtype=loss.dtype)\n        \n        loss*=mask\n\n        return tf.reduce_mean(loss)\n    \n    def train_step(self, inp, targ, hid):\n        loss = 0\n\n        with tf.GradientTape() as tape:\n            encout, enchid = self.encoder(inp, hid)\n\n            decin = tf.expand_dims([self.outmap.word_index['<s>']]*self.batch_size, 1)\n\n            for t in range(1, targ.shape[1]):\n                pred, dec_hid = self.decoder(decin)\n\n                loss += EncDec.loss(self, targ[:,t], pred) # doing loss onto the predicted translation\n\n                decin = tf.expand_dims(targ[:,t], 1) #teacher forcing - feeding in answer as input\n\n            batch_loss = int(loss / targ.shape[1]) # total loss / n_examples = avg loss\n            vs = self.encoder.trainable_variables + self.decoder.trainable_variables\n            \n            grads = tape.gradient(loss, vs) #finds gradient between loss and vars\n            self.optimizer.apply_gradients(zip(grads, vs))\n            \n            return batch_loss\n\n\n    class Encoder(keras.Model):\n        def __init__(self, input_vocab, embed_dim, encoder_units, batch_size):\n            super(EncDec.Encoder, self).__init__()\n            self.embed = layers.Embedding(input_vocab, embed_dim)\n            self.gru = layers.GRU(\n                    encoder_units, \n                    return_sequences=True,\n                    return_state=True,\n                    recurrent_initializer = 'glorot_uniform') # draws samples (initial weights) from uniform distr btwn -lim, lim where lim = sqrt( 6 / (num_inps + num_outs) )\n\n        def call(self, x, hidden):\n            output, state = self.gru(self.embed(x), initial_state=hidden)\n            return output, state\n    \n\n    class Decoder(keras.Model):\n        def __init__(self, target_vocab, embed_dim, units, batch_size):\n            super(EncDec.Decoder, self).__init__()\n            self.batch_size = batch_size\n            self.dec_units = units\n            self.embedding = layers.Embedding(target_vocab, embed_dim)\n            self.gru = layers.GRU(self.dec_units, \n                                   return_sequences=True, \n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n            self.fc = layers.Dense(target_vocab)\n            \n        def call(self, x):\n            x = self.embedding(x)\n            out, state = self.gru(x)\n            out = tf.reshape(out, (-1, out.shape[2]))\n            x = self.fc(out)\n            x = tf.nn.log_softmax(x)\n\n            return x, state\n\ndef hidden_init(batch, n_encoder):\n    return tf.zeros((batch, n_encoder))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.keras as keras\nimport keras.layers as layers\n\nclass EnDe2():\n    def __init__(self, input_vocab, target_vocab, embedding_dim, units, ds, bat):\n        \n        \n        #input -> encoder embedding -> encoder GRU -> save states -> decoder input -> decoder GRU w encoder states -> decoder LSTM -> dense w softmax activ\n        \n        enc_in = layers.Input(shape=(input_vocab,), batch_size=bat)\n        enc_out = layers.Embedding(input_vocab, embedding_dim)(enc_in)\n        enc_out, state = layers.GRU(\n                                            units, \n                                            return_state=True,\n                                            recurrent_initializer='glorot_uniform')(enc_out)\n        \n        dec_in = layers.Input(shape=(target_vocab,), batch_size=bat)\n        dec_out = layers.Embedding(target_vocab, embedding_dim)(dec_in)\n        dec_out = layers.GRU(units)(dec_out, initial_state=state)\n        dec_out = layers.Dense(target_vocab, activation='softmax')(dec_out)\n        \n        self.model = keras.models.Model([enc_in, dec_in], dec_out)\n        \n        self.model.compile(optimizer='adam', loss='categorical_crossentropy')\n        self.model.summary()\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n#import dataset\n#import EncoderDecoder\nfrom termcolor import colored\n#import network\nimport os.path\nimport os\n\n\nprint(colored(\"Successfully imported packages\", \"green\"))\n\nCITATION=\"\"\"\n@inproceedings{\n    Tiedemann2012ParallelData,\n    author = {Tiedemann, J},\n    title = {Parallel Data, Tools and Interfaces in OPUS},\n    booktitle = {LREC}\n    year = {2012}\n}\n\"\"\"\n\n# detect and init the TPU\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#tf.config.experimental_connect_to_cluster(tpu)\n#tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\n#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\n#with tpu_strategy.scope():\n\n# Download the file\n#path_to_zip = tf.keras.utils.get_file(\n#    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n#    extract=True)\n\nen, es, en_map, es_map = getData(\"../input/spaeng/spa.txt\")\n\nen_train = en[0:100000]\nes_train = es[0:100000]\nen_eval = en[100000:]\nes_eval = es[100000:]\n\nBATCH_SIZE=64\nBUFFER_SIZE = len(en_train)\nEMBEDDING_DIM = 512\nSTEP_EPOCH = len(en_train)//BATCH_SIZE\nEPOCHS = 10\nUNITS = 1024\nVOCAB_INP_SIZE = len(en_map.word_index)+1\nVOCAB_OUT_SIZE = len(es_map.word_index)+1\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((en_train, es_train)).shuffle(BUFFER_SIZE)\ntrain_ds = train_ds.batch(BATCH_SIZE, drop_remainder=True)\n\neval_ds = tf.data.Dataset.from_tensor_slices((en_eval, es_eval)).shuffle(BUFFER_SIZE)\neval_ds = eval_ds.batch(BATCH_SIZE, drop_remainder=True)\n\nfull_ds = tf.data.Dataset.from_tensor_slices((en, es)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\n\nprint(colored(\"Train and Eval datasets created\", \"green\"))\n\nprint(detokenize(en_train[0], en_map) + \" | \" + detokenize(es_train[0], es_map))\n\n\"\"\"\nEncDecModel = EncDec(VOCAB_INP_SIZE, VOCAB_OUT_SIZE, EMBEDDING_DIM, UNITS, BATCH_SIZE, en_map, es_map)\nEncDecModel.train(train_ds, EPOCHS, STEP_EPOCH)\ntest = preprocess(\"hi how are you\")\nprint(f\"Preprocessed: {test}\")\ntestseq = en_map.text_to_sequences(test)\nencout, weights = EncDecModel.encoder(testseq, hidden_init(BATCH_SIZE, UNITS))\nend = EncDecModel.decoder(encout)\nprint(f\"Output: {es_map.sequences_to_texts(end)}\")\n\"\"\"\n\nED2 = EnDe2(VOCAB_INP_SIZE, VOCAB_OUT_SIZE, EMBEDDING_DIM, UNITS, full_ds, BATCH_SIZE)\n\n# NMTAttn = network.NMTAttn(VOCAB_INP_SIZE, VOCAB_OUT_SIZE, UNITS, n_encoder=3, n_decoder=3, n_attn_heads=1, dropout=0.03, mode='test')\n# NMTAttn.model(np.array([1, 2, 3]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}