{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install numpy tensorflow --use-feature=2020-resolver termcolor ","execution_count":1,"outputs":[{"output_type":"stream","text":"\nUsage:   \n  /opt/conda/bin/python3.7 -m pip install [options] <requirement specifier> [package-index-options] ...\n  /opt/conda/bin/python3.7 -m pip install [options] -r <requirements file> [package-index-options] ...\n  /opt/conda/bin/python3.7 -m pip install [options] [-e] <vcs project url> ...\n  /opt/conda/bin/python3.7 -m pip install [options] [-e] <local project path> ...\n  /opt/conda/bin/python3.7 -m pip install [options] <archive url/path> ...\n\nno such option: --use-feature\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers as layers\nimport tensorflow.keras as keras\nimport tensorflow as tf\nimport numpy as np\nimport time\nfrom termcolor import colored\n\nclass EncDec():\n\n    def __init__(self, input_vocab, target_vocab, embedding_dim, units, batch_size, inpmap, targmap):\n\n        # constructs network\n        \n        hidden = hidden_init(batch_size, units)\n        self.input_dim = input_vocab\n        self.out_dim = target_vocab\n        self.batch_size = batch_size\n        self.embed_dim = embedding_dim\n        self.units = units\n        self.inmap = inpmap\n        self.outmap = targmap\n\n        self.encoder = EncDec.Encoder(input_vocab, embedding_dim, units, batch_size)\n        self.decoder = EncDec.Decoder(target_vocab, embedding_dim, units, batch_size)\n\n        print(colored(\"Encoder and Decoder models created! Ready for training\", \"green\"))\n\n\n    def train(self, data, epochs, steps_per_epoch):\n        self.optimizer = keras.optimizers.Adam(0.01)\n        self.loss_obj = keras.losses.SparseCategoricalCrossentropy(\n            from_logits=True,\n            reduction='none')\n\n        for ep in range(epochs):\n            start = time.time()\n\n            enchid = hidden_init(self.batch_size, self.units)\n            total_loss = 0\n\n            for (batch, (inp, targ)) in enumerate(data.take(steps_per_epoch)):\n                \n                batch_loss = self.train_step(inp, targ, enchid)\n                total_loss += batch_loss\n\n                if(batch%100 == 0): \n                    print(f\"Epoch {ep+1} | Batch {batch} | Loss {batch_loss}\")\n            \n            print(colored(f\"Epoch {ep+1} completed | Loss {total_loss/steps_per_epoch}\", \"green\"))\n            print(f\"Time for epoch {ep+1}: {time.time()-start} seconds\")\n\n        \n            \n    def loss(self, ans, pred):\n        mask = tf.math.logical_not(tf.math.equal(ans, 0))\n        loss = self.loss_obj(ans, pred)\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss*=mask\n\n        return tf.reduce_mean(loss)\n\n    def train_step(self, inp, targ, hid):\n        loss = 0\n\n        with tf.GradientTape() as tape:\n            encout, enchid = self.encoder(inp, hid)\n\n            decin = tf.expand_dims([self.outmap.word_index['<s>']]*self.batch_size, 1)\n\n            for t in range(1, targ.shape[1]):\n                pred, dec_hid, _ = self.decoder(decin, enchid, encout)\n\n                loss += loss(self, targ[:,t], pred) # doing loss onto the predicted translation\n\n                decin = tf.expand_dims(targ[:,t], 1) #teacher forcing - feeding in answer as input\n\n            batch_loss = int(loss / targ.shape[1]) # total loss / n_examples = avg loss\n            vars = self.encoder.trainable_variables + self.decoder.trainable_variables\n            grads = tape.gradient(loss, vars) #finds gradient between loss and vars\n            self.optimizer.apply_gradients(zip(grads, vars))\n            \n            return grads\n\n\n    class Encoder(keras.Model):\n        def __init__(self, input_vocab, embed_dim, encoder_units, batch_size):\n            super(EncDec.Encoder, self).__init__()\n            self.embed = layers.Embedding(input_vocab, embed_dim)\n            self.gru = layers.GRU(\n                    encoder_units, \n                    return_sequences=True,\n                    return_state=True,\n                    recurrent_initializer = 'glorot_uniform') # draws samples (initial weights) from uniform distr btwn -lim, lim where lim = sqrt( 6 / (num_inps + num_outs) )\n\n        def call(self, x, hidden):\n            output, state = self.gru(self.embed(x), initial_state=hidden)\n            return output, state\n    \n\n    class Decoder(keras.Model):\n        def __init__(self, target_vocab, embed_dim, units, batch_size):\n            super(Decoder, self).__init__()\n            self.batch_size = batch_size\n            self.dec_units = units\n            self.embedding = layers.Embedding(target_vocab, embed_dim)\n            self.gru = layers.GRU(self.dec_units, \n                                   return_sequences=True, \n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n            self.fc = layers.Dense(target_vocab)\n            \n        def call(self, x, enc_out):\n            x = self.embedding(x)\n            out, state = self.gru(x)\n            out = tf.reshape(out, (-1, out.shape[2]))\n            x = self.fc(out)\n            x = tf.nn.log_softmax(x)\n\n            return x, state\n\ndef hidden_init(batch, n_encoder):\n    return tf.zeros((batch, n_encoder))\n\n","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport unicodedata\nimport re\nimport io\n\ndef getData(path):\n    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n    # n_total = 118964\n    pairs = [[preprocess(x) for x in l.split('\\t')] for l in lines]\n    en, sp = zip(*pairs) #make tuples from pairs\n\n    en_tensor, en_map = tokenize(en)\n    sp_tensor, sp_map = tokenize(sp)\n\n    return en_tensor, sp_tensor, en_map, sp_map\n\ndef uni_to_ascii(s) -> str:\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn') # ensuring it's not an accent\n\ndef preprocess(w) -> str:\n    w = uni_to_ascii(w.lower().strip())\n\n    # make space between word and punct\n    w = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", w) # substitutes captured string thing (referenced w \\1) with that thing + space\n    # get rid of multiple space seq things\n    w = re.sub(r'[\" \"]+', \" \", w)\n\n    # keep only letters and punct\n    w = re.sub(r\"[^a-zA-Z?.!,¿¡]+\", \" \", w).strip()\n\n    # add start + end token for model\n    w = '<s> ' + w + ' <e>'\n    \n    return w\n\ndef tokenize(lang) -> (tf.Tensor, tf.keras.preprocessing.text.Tokenizer):\n    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n    lang_tokenizer.fit_on_texts(lang)\n\n    tensor = lang_tokenizer.texts_to_sequences(lang)\n    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n\n    return tensor, lang_tokenizer\n\ndef detokenize(sequence, lang_map) -> str:\n    toRet = \"\"\n    for x in sequence:\n        if x != 0:\n            toRet += lang_map.index_word[x] + \" \"\n    return toRet","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n#import dataset\n#import EncoderDecoder\nfrom termcolor import colored\n#import network\nimport os.path\nimport os\n\n\nprint(colored(\"Successfully imported packages\", \"green\"))\n\nCITATION=\"\"\"\n@inproceedings{\n    Tiedemann2012ParallelData,\n    author = {Tiedemann, J},\n    title = {Parallel Data, Tools and Interfaces in OPUS},\n    booktitle = {LREC}\n    year = {2012}\n}\n\"\"\"\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n\n    # Download the file\n    #path_to_zip = tf.keras.utils.get_file(\n    #    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n    #    extract=True)\n\n    en, es, en_map, es_map = getData(\"../input/spaeng/spa.txt\")\n\n    en_train = en[0:100000]\n    es_train = es[0:100000]\n    en_eval = en[100000:]\n    es_eval = es[100000:]\n\n    BATCH_SIZE=64\n    BUFFER_SIZE = len(en_train)\n    EMBEDDING_DIM = 512\n    STEP_EPOCH = len(en_train)//BATCH_SIZE\n    EPOCHS = 10\n    UNITS = 1024\n    VOCAB_INP_SIZE = len(en_map.word_index)+1\n    VOCAB_OUT_SIZE = len(es_map.word_index)+1\n    \n    train_ds = tf.data.Dataset.from_tensor_slices((en_train, es_train)).shuffle(BUFFER_SIZE)\n    train_ds = train_ds.batch(BATCH_SIZE, drop_remainder=True)\n\n    eval_ds = tf.data.Dataset.from_tensor_slices((en_eval, es_eval)).shuffle(BUFFER_SIZE)\n    eval_ds = eval_ds.batch(BATCH_SIZE, drop_remainder=True)\n\n\n    print(colored(\"Train and Eval datasets created\", \"green\"))\n\n    print(dataset.detokenize(en_train[0], en_map) + \" \" + dataset.detokenize(es_train[0], es_map))\n\n    EncDecModel = EncDec(VOCAB_INP_SIZE, VOCAB_OUT_SIZE, EMBEDDING_DIM, UNITS, BATCH_SIZE, en_map, es_map)\n    EncDecModel.train(self, train_ds, EPOCHS, STEP_EPOCH)\n\n\n    # NMTAttn = network.NMTAttn(VOCAB_INP_SIZE, VOCAB_OUT_SIZE, UNITS, n_encoder=3, n_decoder=3, n_attn_heads=1, dropout=0.03, mode='test')\n    # NMTAttn.model(np.array([1, 2, 3]))","execution_count":10,"outputs":[{"output_type":"stream","text":"\u001b[32mSuccessfully imported packages\u001b[0m\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'BUFFER_SIZE' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-1527bebca374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mes_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'BUFFER_SIZE' is not defined"]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}